{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFJlSEUhxAh3",
        "outputId": "f055bf5f-219d-4951-c021-33a7129b05cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "DTA-TRAIN.ipynb\n",
        "\n",
        "This script implements a deep learning model to predict protein binding sites\n",
        "based on structural and sequence-based information.\n",
        "\n",
        "Architecture Overview:\n",
        "1.  Inputs: Three matrices - AlphaFold distance map, a PLM-derived matrix (A*A^T),\n",
        "    and an AlphaFold PAE matrix.\n",
        "2.  Feature Extraction: Each input is processed by an initial 2D convolution block.\n",
        "3.  Core Interaction Block (x3): The model contains three repeating blocks. Each block consists of:\n",
        "    - 2D Convolutions for each of the three feature streams.\n",
        "    - Cross-attention between the distance map and PLM streams.\n",
        "4.  Attention Layers: After the core blocks, axial attention is applied separately to the\n",
        "    distance and PLM feature maps.\n",
        "5.  Prediction Head:\n",
        "    - The resulting feature maps are concatenated.\n",
        "    - A projection layer processes the combined features.\n",
        "    - Row-wise pooling generates per-residue features.\n",
        "    - Two separate heads produce the final outputs:\n",
        "        a) Per-residue binding probability (a vector of 0s and 1s).\n",
        "        b) A global protein-level binding prediction (a single 0 or 1).\n",
        "6.  Loss Function: A combined loss is used, incorporating Focal Loss and Dice Loss for the\n",
        "    per-residue prediction and BCE loss for the global prediction.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "from typing import List, Tuple\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import logging\n",
        "\n",
        "# Set up basic logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Data Loading and Preprocessing\n",
        "# -----------------------------\n",
        "class CLAPEDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset to load AlphaFold distance maps, PLM-derived matrices, PAE matrices,\n",
        "    and corresponding binding site labels.\n",
        "    \"\"\"\n",
        "    def __init__(self, dist_dir: str, plm_dir: str, pae_dir: str,\n",
        "                 label_csv: str, file_list: List[str] = None, max_len: int = 1200):\n",
        "        self.dist_dir = dist_dir\n",
        "        self.plm_dir = plm_dir\n",
        "        self.pae_dir = pae_dir\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # --- Load and process binding site labels ---\n",
        "        try:\n",
        "            self.label_df = pd.read_csv(label_csv)\n",
        "            self.labels = self._group_labels(self.label_df)\n",
        "            logging.info(f\"Successfully loaded and processed labels from {label_csv}\")\n",
        "        except FileNotFoundError:\n",
        "            logging.error(f\"Label file not found at {label_csv}\")\n",
        "            raise\n",
        "\n",
        "        # --- Collect all available protein identifiers (basenames) ---\n",
        "        if file_list:\n",
        "            self.basenames = file_list\n",
        "        else:\n",
        "            dist_files = glob.glob(os.path.join(dist_dir, \"*_distances.csv\"))\n",
        "            # Extracts 'O00141-F1-model_v4' from 'AF-O00141-F1-model_v4_distances.csv'\n",
        "            self.basenames = [os.path.basename(p).split('_distances.csv')[0].replace(\"AF-\", \"\") for p in dist_files]\n",
        "\n",
        "        logging.info(f\"Dataset initialized with {len(self.basenames)} samples.\")\n",
        "\n",
        "    def _group_labels(self, df: pd.DataFrame) -> dict:\n",
        "        \"\"\"\n",
        "        Groups binding site ranges by uniprot_id.\n",
        "        Example output: {'O00141': [(104, 112), (127, 127)], ...}\n",
        "        \"\"\"\n",
        "        grouped = {}\n",
        "        for uid, sub_df in df.groupby(\"uniprot_id\"):\n",
        "            ranges = []\n",
        "            for _, row in sub_df.iterrows():\n",
        "                if pd.notna(row[\"start\"]) and pd.notna(row[\"end\"]):\n",
        "                    try:\n",
        "                        # Convert to int and ensure start <= end\n",
        "                        s, e = int(row[\"start\"]), int(row[\"end\"])\n",
        "                        ranges.append((min(s, e), max(s, e)))\n",
        "                    except ValueError:\n",
        "                        logging.warning(f\"Could not parse start/end for {uid}: {row['start']}, {row['end']}\")\n",
        "            if ranges:\n",
        "                grouped[uid] = ranges\n",
        "        return grouped\n",
        "\n",
        "    def _make_label_vector(self, uniprot_id: str, n: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Creates a binary vector indicating binding sites for a given protein length.\n",
        "        Residues in binding regions are marked as 1, others as 0.\n",
        "        \"\"\"\n",
        "        label_vec = np.zeros(n, dtype=np.float32)\n",
        "        if uniprot_id not in self.labels:\n",
        "            return label_vec\n",
        "\n",
        "        for (start, end) in self.labels[uniprot_id]:\n",
        "            # Convert 1-based start/end from CSV to 0-based index\n",
        "            s_idx, e_idx = max(0, start - 1), min(n - 1, end - 1)\n",
        "            if s_idx <= e_idx:\n",
        "                label_vec[s_idx : e_idx + 1] = 1\n",
        "        return label_vec\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.basenames)\n",
        "\n",
        "    def _read_matrix(self, path: str, delimiter=\",\") -> np.ndarray:\n",
        "        \"\"\"Reads a matrix from file, handling potential errors.\"\"\"\n",
        "        if not os.path.exists(path):\n",
        "            logging.error(f\"File not found at path: {path}\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            if delimiter == 'json':\n",
        "                with open(path, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "                # Handle PAE JSON structure\n",
        "                if isinstance(data, list) and data:\n",
        "                    data = data[0]\n",
        "                arr = np.array(data.get(\"predicted_aligned_error\", []), dtype=np.float32)\n",
        "            elif delimiter == ',':\n",
        "                # Use genfromtxt for CSVs as it is robust to missing/empty values\n",
        "                arr = np.genfromtxt(path, delimiter=delimiter, dtype=np.float32, filling_values=0.0)\n",
        "            else:\n",
        "                 arr = np.loadtxt(path, delimiter=delimiter, dtype=np.float32)\n",
        "\n",
        "            # Replace any NaN/inf values with 0\n",
        "            return np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to read or parse matrix {path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _pad_or_truncate(self, arr: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Pads or truncates a matrix/vector to self.max_len.\"\"\"\n",
        "        if arr is None:\n",
        "            return None\n",
        "\n",
        "        if arr.ndim == 2:\n",
        "            n, _ = arr.shape\n",
        "            if n > self.max_len:\n",
        "                return arr[:self.max_len, :self.max_len]\n",
        "            elif n < self.max_len:\n",
        "                pad_width = ((0, self.max_len - n), (0, self.max_len - n))\n",
        "                return np.pad(arr, pad_width, mode='constant', constant_values=0)\n",
        "        elif arr.ndim == 1:\n",
        "            n = arr.shape[0]\n",
        "            if n > self.max_len:\n",
        "                return arr[:self.max_len]\n",
        "            elif n < self.max_len:\n",
        "                pad_width = (0, self.max_len - n)\n",
        "                return np.pad(arr, pad_width, mode='constant', constant_values=0)\n",
        "        return arr\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        base = self.basenames[idx]\n",
        "        uniprot_id = base.split(\"-\")[0]\n",
        "\n",
        "        # --- Construct file paths robustly ---\n",
        "        dist_path_list = glob.glob(os.path.join(self.dist_dir, f\"AF-{base}*_distances.csv\"))\n",
        "        plm_path_list = glob.glob(os.path.join(self.plm_dir, f\"{base}*M_transformed.txt\"))\n",
        "        pae_path_list = glob.glob(os.path.join(self.pae_dir, f\"{uniprot_id}*_pae.txt\"))\n",
        "\n",
        "        # --- Check if files were found ---\n",
        "        if not dist_path_list or not plm_path_list or not pae_path_list:\n",
        "            logging.warning(f\"Skipping sample {base} because at least one file was not found.\")\n",
        "            return (torch.zeros(1, self.max_len, self.max_len),\n",
        "                    torch.zeros(1, self.max_len, self.max_len),\n",
        "                    torch.zeros(1, self.max_len, self.max_len),\n",
        "                    torch.zeros(self.max_len),\n",
        "                    torch.tensor(0.0), base + \"_error\")\n",
        "\n",
        "        # --- Load data ---\n",
        "        dist = self._read_matrix(dist_path_list[0], delimiter=\",\")\n",
        "        plm = self._read_matrix(plm_path_list[0], delimiter=\" \")\n",
        "        pae = self._read_matrix(pae_path_list[0], delimiter=\"json\")\n",
        "\n",
        "        if dist is None or plm is None or pae is None:\n",
        "            logging.error(f\"Skipping sample {base} due to a file reading error.\")\n",
        "            # Return a dummy sample to avoid crashing the DataLoader\n",
        "            return (torch.zeros(1, self.max_len, self.max_len),\n",
        "                    torch.zeros(1, self.max_len, self.max_len),\n",
        "                    torch.zeros(1, self.max_len, self.max_len),\n",
        "                    torch.zeros(self.max_len),\n",
        "                    torch.tensor(0.0), base + \"_error\")\n",
        "\n",
        "\n",
        "        # --- Unify matrix sizes ---\n",
        "        n = dist.shape[0]\n",
        "        if not (plm.shape[0] == n and pae.shape[0] == n):\n",
        "            logging.warning(f\"Matrix size mismatch for {base}. dist:{dist.shape}, plm:{plm.shape}, pae:{pae.shape}. Truncating to smallest dimension.\")\n",
        "            min_n = min(n, plm.shape[0], pae.shape[0])\n",
        "            dist, plm, pae = dist[:min_n, :min_n], plm[:min_n, :min_n], pae[:min_n, :min_n]\n",
        "            n = min_n\n",
        "\n",
        "        # --- Create label vector ---\n",
        "        label_vec = self._make_label_vector(uniprot_id, n)\n",
        "        global_label = float(label_vec.sum() > 0)\n",
        "\n",
        "        # --- Pad all matrices and labels to max_len ---\n",
        "        dist_padded = self._pad_or_truncate(dist)\n",
        "        plm_padded = self._pad_or_truncate(plm)\n",
        "        pae_padded = self._pad_or_truncate(pae)\n",
        "        label_vec_padded = self._pad_or_truncate(label_vec)\n",
        "\n",
        "        # --- Convert to Tensors and add channel dimension ---\n",
        "        dist_tensor = torch.from_numpy(dist_padded).float().unsqueeze(0)\n",
        "        plm_tensor = torch.from_numpy(plm_padded).float().unsqueeze(0)\n",
        "        pae_tensor = torch.from_numpy(pae_padded).float().unsqueeze(0)\n",
        "        label_tensor = torch.from_numpy(label_vec_padded).float()\n",
        "\n",
        "        return (\n",
        "            dist_tensor,      # [1, max_len, max_len]\n",
        "            plm_tensor,       # [1, max_len, max_len]\n",
        "            pae_tensor,       # [1, max_len, max_len]\n",
        "            label_tensor,     # [max_len]\n",
        "            torch.tensor(global_label, dtype=torch.float32),\n",
        "            base\n",
        "        )\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Model Building Blocks\n",
        "# -----------------------------\n",
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"A block of two 2D convolutions with batch norm and ReLU.\"\"\"\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, padding=1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, padding=padding, bias=False),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, kernel_size=kernel_size, padding=padding, bias=False),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class CrossAttention2D(nn.Module):\n",
        "    \"\"\"Cross-attention between two 2D feature maps (e.g., dist_map and plm_map).\"\"\"\n",
        "    def __init__(self, channels: int, num_heads: int = 8):\n",
        "        super().__init__()\n",
        "        self.mha = nn.MultiheadAttention(embed_dim=channels, num_heads=num_heads, batch_first=True, dropout=0.1)\n",
        "        self.norm_q = nn.LayerNorm(channels)\n",
        "        self.norm_kv = nn.LayerNorm(channels)\n",
        "    def forward(self, query, key_value):\n",
        "        B, C, H, W = query.shape\n",
        "        q = query.flatten(2).permute(0, 2, 1)    # (B, H*W, C)\n",
        "        kv = key_value.flatten(2).permute(0, 2, 1) # (B, H*W, C)\n",
        "        q_norm = self.norm_q(q)\n",
        "        kv_norm = self.norm_kv(kv)\n",
        "        attn_out, _ = self.mha(q_norm, kv_norm, kv_norm)\n",
        "        return attn_out.permute(0, 2, 1).view(B, C, H, W)\n",
        "\n",
        "class AxialAttention(nn.Module):\n",
        "    \"\"\"Axial attention applying self-attention first along rows, then columns.\"\"\"\n",
        "    def __init__(self, channels: int, num_heads: int = 8):\n",
        "        super().__init__()\n",
        "        self.row_attn = nn.MultiheadAttention(embed_dim=channels, num_heads=num_heads, batch_first=True)\n",
        "        self.col_attn = nn.MultiheadAttention(embed_dim=channels, num_heads=num_heads, batch_first=True)\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        # Row attention\n",
        "        x_rows = x.permute(0, 2, 3, 1).reshape(B * H, W, C) # Treat each row as a sequence\n",
        "        x_rows = self.norm(x_rows)\n",
        "        out_rows, _ = self.row_attn(x_rows, x_rows, x_rows)\n",
        "        out_rows = out_rows.reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
        "        # Column attention\n",
        "        x_cols = x.permute(0, 3, 2, 1).reshape(B * W, H, C) # Treat each column as a sequence\n",
        "        x_cols = self.norm(x_cols)\n",
        "        out_cols, _ = self.col_attn(x_cols, x_cols, x_cols)\n",
        "        out_cols = out_cols.reshape(B, W, H, C).permute(0, 3, 2, 1)\n",
        "        return out_rows + out_cols\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Main Model Architecture\n",
        "# -----------------------------\n",
        "class BindingAffinityNet(nn.Module):\n",
        "    def __init__(self, base_channels: int = 16, num_heads: int = 4, K: int = 5):\n",
        "        super().__init__()\n",
        "        self.K = K  # Store hyperparameter K as requested\n",
        "\n",
        "        # --- Initial Encoders ---\n",
        "        self.conv_dist = ConvBlock(1, base_channels)\n",
        "        self.conv_plm = ConvBlock(1, base_channels)\n",
        "        self.conv_pae = ConvBlock(1, base_channels)\n",
        "\n",
        "        # --- 3x Repeated Interaction Blocks ---\n",
        "        self.repeats = nn.ModuleList()\n",
        "        for _ in range(3):\n",
        "            block = nn.ModuleDict({\n",
        "                'conv_d': ConvBlock(base_channels, base_channels),\n",
        "                'conv_p': ConvBlock(base_channels, base_channels),\n",
        "                'cross_dp': CrossAttention2D(base_channels, num_heads),\n",
        "                'cross_pd': CrossAttention2D(base_channels, num_heads),\n",
        "            })\n",
        "            self.repeats.append(block)\n",
        "\n",
        "        # --- Final Attention Layers ---\n",
        "        self.axial_dist = AxialAttention(base_channels, num_heads)\n",
        "        self.axial_plm = AxialAttention(base_channels, num_heads)\n",
        "\n",
        "        # --- Prediction Head ---\n",
        "        # Combines the two streams\n",
        "        final_channels = base_channels * 2\n",
        "        self.proj = nn.Conv2d(final_channels, final_channels, kernel_size=1)\n",
        "\n",
        "        # Head for per-residue prediction\n",
        "        self.head_residue = nn.Sequential(\n",
        "            nn.Conv1d(final_channels, base_channels, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(base_channels, 1, kernel_size=1),\n",
        "        )\n",
        "        # Head for global protein prediction\n",
        "        self.head_global = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(final_channels, base_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(base_channels, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, dist, plm, pae):\n",
        "        # Initial encoding\n",
        "        d = self.conv_dist(dist)\n",
        "        p = self.conv_plm(plm)\n",
        "        # PAE is used as an auxiliary feature map, added to dist and plm\n",
        "        pae_feats = self.conv_pae(pae)\n",
        "        d, p = d + pae_feats, p + pae_feats\n",
        "\n",
        "        # 3x Interaction Blocks\n",
        "        for block in self.repeats:\n",
        "            d_res, p_res = d, p\n",
        "            d, p = block['conv_d'](d), block['conv_p'](p)\n",
        "            d_att = block['cross_dp'](d, p) # dist queries plm\n",
        "            p_att = block['cross_pd'](p, d) # plm queries dist\n",
        "            d, p = d + d_att, p + p_att\n",
        "            # Add residuals\n",
        "            d, p = d + d_res, p + p_res\n",
        "\n",
        "        # Final Axial Attention\n",
        "        d_ax = self.axial_dist(d)\n",
        "        p_ax = self.axial_plm(p)\n",
        "\n",
        "        # Concatenate and project for prediction\n",
        "        concat = torch.cat([d_ax, p_ax], dim=1)\n",
        "        x = F.relu(self.proj(concat))\n",
        "\n",
        "        # --- Generate Outputs ---\n",
        "        # 1. Per-residue prediction via row-wise pooling\n",
        "        row_features = x.mean(dim=3)  # Avg pool across columns -> (B, C, H)\n",
        "        residue_logits = self.head_residue(row_features).squeeze(1) # (B, H)\n",
        "\n",
        "        # 2. Global prediction\n",
        "        global_logit = self.head_global(x).squeeze(1) # (B,)\n",
        "\n",
        "        return residue_logits, global_logit\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Loss Functions\n",
        "# -----------------------------\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha, self.gamma, self.reduction = alpha, gamma, reduction\n",
        "    def forward(self, logits, targets):\n",
        "        bce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
        "        pt = torch.exp(-bce_loss)\n",
        "        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
        "        focal_loss = alpha_t * (1 - pt)**self.gamma * bce_loss\n",
        "        return focal_loss.mean() if self.reduction == 'mean' else focal_loss.sum()\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1.0):\n",
        "        super().__init__()\n",
        "        self.smooth = smooth\n",
        "    def forward(self, logits, targets):\n",
        "        probs = torch.sigmoid(logits)\n",
        "        intersection = (probs * targets).sum(dim=1)\n",
        "        union = probs.sum(dim=1) + targets.sum(dim=1)\n",
        "        dice_score = (2. * intersection + self.smooth) / (union + self.smooth)\n",
        "        return 1. - dice_score.mean()\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    \"\"\"Combines Focal, Dice, and BCE losses for the two model outputs.\"\"\"\n",
        "    def __init__(self, w_focal=0.7, w_dice=0.3, w_global=0.5):\n",
        "        super().__init__()\n",
        "        self.focal_loss = FocalLoss()\n",
        "        self.dice_loss = DiceLoss()\n",
        "        self.global_loss_fn = nn.BCEWithLogitsLoss()\n",
        "        self.w_focal, self.w_dice, self.w_global = w_focal, w_dice, w_global\n",
        "\n",
        "    def forward(self, residue_logits, global_logits, residue_targets, global_targets):\n",
        "        focal = self.focal_loss(residue_logits, residue_targets)\n",
        "        dice = self.dice_loss(residue_logits, residue_targets)\n",
        "        residue_loss = self.w_focal * focal + self.w_dice * dice\n",
        "        global_loss = self.global_loss_fn(global_logits, global_targets)\n",
        "        total_loss = residue_loss + self.w_global * global_loss\n",
        "\n",
        "        stats = {'loss': total_loss.item(), 'focal': focal.item(), 'dice': dice.item(), 'global_bce': global_loss.item()}\n",
        "        return total_loss, stats\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Training and Evaluation\n",
        "# -----------------------------\n",
        "def train_one_epoch(model, dataloader, optimizer, loss_fn, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        dist, plm, pae, res_labels, glob_labels, _ = batch\n",
        "        # Filter out error samples\n",
        "        if dist.nelement() == 0: continue\n",
        "\n",
        "        # Move batch to device\n",
        "        dist, plm, pae = dist.to(device), plm.to(device), pae.to(device)\n",
        "        res_labels, glob_labels = res_labels.to(device), glob_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        residue_logits, global_logits = model(dist, plm, pae)\n",
        "        loss, stats = loss_fn(residue_logits, global_logits, res_labels, glob_labels)\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            logging.warning(f\"NaN loss detected at batch {i}. Skipping update.\")\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if i % 20 == 0:\n",
        "            logging.info(f\"Batch {i}/{len(dataloader)} - {stats}\")\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def verify_labelling(dataset: CLAPEDataset, uniprot_id: str, protein_length: int):\n",
        "    \"\"\"\n",
        "    A sanity-check function to verify the label creation for a specific protein.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"--- Verifying Labels for UniProt ID: {uniprot_id} ---\")\n",
        "\n",
        "    # 1. Find the original ranges from the dataframe\n",
        "    original_ranges = dataset.labels.get(uniprot_id)\n",
        "    if not original_ranges:\n",
        "        print(f\"No binding site entries found for {uniprot_id} in the CSV.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(original_ranges)} binding site range(s) in CSV (1-based): {original_ranges}\")\n",
        "\n",
        "    # 2. Create the label vector\n",
        "    label_vector = dataset._make_label_vector(uniprot_id, protein_length)\n",
        "\n",
        "    # 3. Find the indices that are marked as '1'\n",
        "    labeled_indices = np.where(label_vector == 1)[0]\n",
        "\n",
        "    # Convert 0-based indices back to 1-based for easy comparison\n",
        "    labeled_indices_1_based = labeled_indices + 1\n",
        "\n",
        "    print(f\"Protein Length Used: {protein_length}\")\n",
        "    print(f\"Generated Label Vector has {len(labeled_indices)} '1's.\")\n",
        "    print(f\"Indices labeled as binding sites (1-based):\")\n",
        "    print(labeled_indices_1_based.tolist())\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Main Execution Block\n",
        "# -----------------------------\n",
        "if __name__ == '__main__':\n",
        "    # --- Configuration ---\n",
        "    # NOTE: Adjust this path to your Google Drive folder structure\n",
        "    DRIVE_PATH = '/content/drive/MyDrive/CLAPE-RESULTS'\n",
        "\n",
        "    # Check if paths exist\n",
        "    if not os.path.exists(DRIVE_PATH):\n",
        "        raise FileNotFoundError(f\"The base directory {DRIVE_PATH} was not found. \"\n",
        "                              \"Please mount your Google Drive or update the path.\")\n",
        "\n",
        "    DIST_DIR = os.path.join(DRIVE_PATH, 'af-distancemaps')\n",
        "    PLM_DIR = os.path.join(DRIVE_PATH, 'transformed_matrices')\n",
        "    PAE_DIR = os.path.join(DRIVE_PATH, 'PAE-MATRICES')\n",
        "    LABEL_CSV = os.path.join(DRIVE_PATH, 'binding_sites_uniprot.csv')\n",
        "\n",
        "    BATCH_SIZE = 2\n",
        "    MAX_LEN = 1024 # Max sequence length for padding/truncating\n",
        "    LEARNING_RATE = 1e-4\n",
        "    EPOCHS = 10\n",
        "\n",
        "    # --- Data Setup ---\n",
        "    logging.info(\"Setting up dataset...\")\n",
        "    dataset = CLAPEDataset(\n",
        "        dist_dir=DIST_DIR,\n",
        "        plm_dir=PLM_DIR,\n",
        "        pae_dir=PAE_DIR,\n",
        "        label_csv=LABEL_CSV,\n",
        "        max_len=MAX_LEN\n",
        "    )\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # --- VERIFY LABELLING ---\n",
        "    # You can change 'O00141' to any UniProt ID from your dataset\n",
        "    # and provide its approximate length to check the labels.\n",
        "    verify_labelling(dataset, uniprot_id='O00141', protein_length=360)\n",
        "    verify_labelling(dataset, uniprot_id='O14920', protein_length=470)\n",
        "\n",
        "\n",
        "    # --- Model Setup ---\n",
        "    logging.info(\"Setting up model...\")\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = BindingAffinityNet(base_channels=16, num_heads=4, K=5).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    loss_fn = CombinedLoss()\n",
        "\n",
        "    logging.info(f\"Starting training on {device} for {EPOCHS} epochs.\")\n",
        "\n",
        "    # --- Training Loop ---\n",
        "    for epoch in range(EPOCHS):\n",
        "        avg_loss = train_one_epoch(model, dataloader, optimizer, loss_fn, device)\n",
        "        logging.info(f\"--- Epoch {epoch+1}/{EPOCHS} | Average Loss: {avg_loss:.4f} ---\")\n",
        "\n",
        "        # TODO: Add a validation loop here\n",
        "        # TODO: Save model checkpoints\n",
        "\n",
        "    logging.info(\"Training finished.\")\n",
        "\n",
        "    # --- Example of making a prediction ---\n",
        "    logging.info(\"Running a sample prediction...\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get a single batch for inference\n",
        "        try:\n",
        "            sample_batch = next(iter(dataloader))\n",
        "            dist, plm, pae, res_labels, _, names = sample_batch\n",
        "\n",
        "            if dist.nelement() > 0:\n",
        "                dist, plm, pae = dist.to(device), plm.to(device), pae.to(device)\n",
        "\n",
        "                res_logits, glob_logits = model(dist, plm, pae)\n",
        "\n",
        "                # Get probabilities\n",
        "                res_probs = torch.sigmoid(res_logits)\n",
        "                glob_probs = torch.sigmoid(glob_logits)\n",
        "\n",
        "                print(f\"\\nSample Prediction for protein: {names[0]}\")\n",
        "                print(f\"Global binding probability: {glob_probs[0].item():.3f}\")\n",
        "                # Find residue indices with high predicted probability\n",
        "                high_prob_indices = (res_probs[0] > 0.5).nonzero(as_tuple=True)[0]\n",
        "                print(f\"Residues with >50% binding probability: {high_prob_indices.cpu().numpy().tolist()}\")\n",
        "                print(f\"Actual binding sites (first few): {(res_labels[0] > 0.5).nonzero(as_tuple=True)[0].numpy().tolist()[:15]}\")\n",
        "        except StopIteration:\n",
        "            logging.warning(\"Could not get a sample batch for prediction, the dataloader is empty.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNFBTkGw0TQ6",
        "outputId": "546fae1e-a51f-4dd0-8765-dbd63ab018a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "--- Verifying Labels for UniProt ID: O00141 ---\n",
            "Found 2 binding site range(s) in CSV (1-based): [(104, 112), (127, 127)]\n",
            "Protein Length Used: 360\n",
            "Generated Label Vector has 10 '1's.\n",
            "Indices labeled as binding sites (1-based):\n",
            "[104, 105, 106, 107, 108, 109, 110, 111, 112, 127]\n",
            "==================================================\n",
            "\n",
            "\n",
            "==================================================\n",
            "--- Verifying Labels for UniProt ID: O14920 ---\n",
            "Found 2 binding site range(s) in CSV (1-based): [(21, 29), (44, 44)]\n",
            "Protein Length Used: 470\n",
            "Generated Label Vector has 10 '1's.\n",
            "Indices labeled as binding sites (1-based):\n",
            "[21, 22, 23, 24, 25, 26, 27, 28, 29, 44]\n",
            "==================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "WARNING:root:Matrix size mismatch for P21802-F1-model_v4. dist:(822, 822), plm:(821, 821), pae:(821, 821). Truncating to smallest dimension.\n",
            "WARNING:root:Matrix size mismatch for P53778-F1-model_v4. dist:(368, 368), plm:(367, 367), pae:(367, 367). Truncating to smallest dimension.\n",
            "WARNING:root:Matrix size mismatch for P04049-F1-model_v4. dist:(649, 649), plm:(648, 648), pae:(648, 648). Truncating to smallest dimension.\n"
          ]
        }
      ]
    }
  ]
}